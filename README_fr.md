\
> [!NOTE]
> Remarque : Ce document est une version traduite automatiquement et peut contenir des inexactitudes. Vos contributions pour am√©liorer la traduction sont les bienvenues !

<!-- SPDX-License-Identifier: Apache-2.0 OR CC BY 4.0 OR other -->
<div align="center"><b><a href="README.md">English</a> | <a href="README_zh.md">ÁÆÄ‰Ωì‰∏≠Êñá</a> | <a href="README_ja.md">Êó•Êú¨Ë™û</a> | <a href="README_es.md">Espa√±ol</a> | <a href="README_fr.md">Fran√ßais</a></b></div>

<h1 align="center" style="border: none">
    <div style="border: none">
        <!-- Si vous avez un logo, vous pouvez l'ajouter ici. Exemple :
        <a href="YOUR_PROJECT_LINK"><picture>
            <source media="(prefers-color-scheme: dark)" srcset="PATH_TO_DARK_LOGO.svg">
            <source media="(prefers-color-scheme: light)" srcset="PATH_TO_LIGHT_LOGO.svg">
            <img alt="Logo du Projet" src="PATH_TO_LIGHT_LOGO.svg" width="200" />
        </picture></a>
        <br>
        -->
        Tiny QA Benchmark++ (TQB++)
    </div>
</h1>

<p align="center">
Un jeu de donn√©es d'√©valuation ultra-l√©ger et un g√©n√©rateur synth√©tique <br>pour exposer les d√©faillances critiques des LLM en quelques secondes, id√©al pour CI/CD et LLMOps.
</p>

<div align="center">
    <a href="https://pypi.org/project/tinyqabenchmarkpp/"><img alt="Version PyPI" src="https://img.shields.io/pypi/v/tinyqabenchmarkpp"></a>
    <a href="https://github.com/vincentkoc/tiny_qa_benchmark_pp/blob/main/LICENSE"><img alt="Licence" src="https://img.shields.io/badge/Apache-2.0-green"></a>
    <a href="https://huggingface.co/datasets/vincentkoc/tiny_qa_benchmark_pp"><img alt="Jeu de donn√©es Hugging Face" src="https://img.shields.io/badge/ü§ó%20Dataset-Tiny%20QA%20Benchmark%2B%2B-blue"></a>
    <a href="https://arxiv.org/abs/2505.12058"><img alt="arXiv" src="https://img.shields.io/badge/arXiv-2505.12058-b31b1b.svg"></a>
    <!-- Envisagez d'ajouter un badge de workflow GitHub Actions si vous avez configur√© la CI -->
    <!-- ex: <a href="YOUR_WORKFLOW_LINK"><img alt="√âtat de la construction" src="YOUR_WORKFLOW_BADGE_SVG_LINK"></a> -->
</div>

<p align="center">
    <a href="https://github.com/vincentkoc/tiny_qa_benchmark_pp"><b>GitHub</b></a> ‚Ä¢
    <a href="https://huggingface.co/datasets/vincentkoc/tiny_qa_benchmark_pp"><b>Jeu de donn√©es Hugging Face</b></a> ‚Ä¢
    <a href="https://arxiv.org/abs/2505.12058"><b>Article (arXiv:2505.12058)</b></a> ‚Ä¢
    <a href="https://pypi.org/project/tinyqabenchmarkpp/"><b>PyPI</b></a>
</p>

<hr>
<!-- Optionnel : Si vous avez une miniature de projet, vous pouvez l'ajouter ici -->
<!-- <p align="center"><img alt="Miniature TQB++" src="path/to/your/thumbnail.png" width="700"></p> -->

**Tiny QA Benchmark++ (TQB++)** est une suite d'√©valuation ultra-l√©g√®re et un package Python con√ßu pour exposer les d√©faillances critiques des syst√®mes de grands mod√®les de langage (LLM) en quelques secondes. Il sert de tests unitaires logiciels pour LLM, id√©al pour les v√©rifications rapides CI/CD, l'ing√©nierie des prompts et l'assurance qualit√© continue dans les LLMOps modernes, √† utiliser parall√®lement aux outils d'√©valuation LLM existants tels que [Opik](https://github.com/comet-ml/opik/).

Ce d√©p√¥t contient l'impl√©mentation officielle et les jeux de donn√©es synth√©tiques pour l'article : *Tiny QA Benchmark++: Micro Gold Dataset with Synthetic Multilingual Generation for Rapid LLMOps Smoke Tests*.

**Article :** (Les d√©tails et le lien vers la pr√©publication seront fournis ici une fois publi√©s)

- **Hugging Face Hub :** [datasets/vincentkoc/tiny_qa_benchmark_pp](https://huggingface.co/datasets/vincentkoc/tiny_qa_benchmark_pp)
- **D√©p√¥t GitHub :** [vincentkoc/tiny_qa_benchmark_pp](https://github.com/vincentkoc/tiny_qa_benchmark_pp)

## Fonctionnalit√©s Cl√©s

*   **Noyau Standard Or Immuable :** Un jeu de donn√©es de questions-r√©ponses (QA) en anglais de 52 √©l√©ments √©labor√© √† la main (`core_en`) pour les tests de r√©gression d√©terministes, issu de [datasets/vincentkoc/tiny_qa_benchmark](https://huggingface.co/datasets/vincentkoc/tiny_qa_benchmark).
*   **Bo√Æte √† Outils de Personnalisation Synth√©tique :** Un script Python (`tools/generator`) utilisant LiteLLM pour g√©n√©rer des micro-benchmarks sur mesure √† la demande pour n'importe quelle langue, sujet ou difficult√©.
*   **M√©tadonn√©es Normalis√©es :** Artefacts empaquet√©s au format Croissant JSON-LD (`metadata/`) pour la d√©couvrabilit√© et le chargement automatique par les outils et les moteurs de recherche.
*   **Science Ouverte :** Tout le code (g√©n√©rateur, scripts d'√©valuation) et le jeu de donn√©es principal en anglais sont publi√©s sous la licence Apache-2.0. Les packs de donn√©es g√©n√©r√©s synth√©tiquement ont une licence personnalis√©e √† des fins d'√©valuation uniquement.
*   **Alignement LLMOps :** Con√ßu pour une int√©gration facile dans les pipelines CI/CD, les flux de travail d'ing√©nierie des prompts, la d√©tection de d√©rive multilingue et les tableaux de bord d'observabilit√©.
*   **Packs Multilingues :** Packs pr√©construits pour de nombreuses langues, dont l'anglais, le fran√ßais, l'espagnol, le portugais, l'allemand, le chinois, le japonais, le turc, l'arabe et le russe.

## Utilisation du Package Python `tinyqabenchmarkpp`

Les capacit√©s principales de g√©n√©ration synth√©tique de TQB++ sont disponibles sous forme de package Python, `tinyqabenchmarkpp`, installable depuis PyPI.

### Installation

```bash
pip install tinyqabenchmarkpp
```

(Remarque : Assurez-vous que Python 3.8+ et pip sont install√©s. Le nom exact du package sur PyPI peut varier ; veuillez consulter la [page officielle du projet PyPI](https://pypi.org/project/tinyqabenchmarkpp/) pour le nom correct du package si cette commande ne fonctionne pas.)

### G√©n√©ration de Jeux de Donn√©es Synth√©tiques via CLI

Une fois install√©, vous pouvez utiliser la commande `tinyqabenchmarkpp` (ou `python -m tinyqabenchmarkpp.generate`) pour cr√©er des jeux de donn√©es QA personnalis√©s.

**Exemple :**
```bash
tinyqabenchmarkpp --num 10 --languages "en,es" --categories "science" --output-file "./science_pack.jsonl"
```

Cela g√©n√©rera un petit pack de 10 questions scientifiques en anglais et en espagnol.

Pour des instructions d√©taill√©es sur tous les param√®tres disponibles (comme `--model`, `--context`, `--difficulty`, etc.), l'utilisation avanc√©e et des exemples pour diff√©rents fournisseurs LLM (OpenAI, OpenRouter, Ollama), veuillez consulter le **[README de la Bo√Æte √† Outils du G√©n√©rateur](tools/generator/README.md)** ou ex√©cuter `tinyqabenchmarkpp --help`.

Bien que le package `tinyqabenchmarkpp` se concentre sur la *g√©n√©ration* de jeux de donn√©es, le projet TQB++ fournit √©galement des jeux de donn√©es pr√©-g√©n√©r√©s et des outils d'√©valuation, comme d√©crit ci-dessous.

## Chargement de Jeux de Donn√©es avec `datasets` de Hugging Face

Les jeux de donn√©es TQB++ sont disponibles sur le Hugging Face Hub et –º–æ–≥—É—Ç –±—ã—Ç—å –ª–µ–≥–∫–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ `datasets`. C'est la m√©thode recommand√©e pour acc√©der aux donn√©es.

```python
from datasets import load_dataset, get_dataset_config_names

# D√©couvrir les configurations de jeux de donn√©es disponibles (ex. core_en, pack_fr_40, etc.)
configs = get_dataset_config_names("vincentkoc/tiny_qa_benchmark_pp")
print(f"Configurations disponibles : {configs}")

# Charger le jeu de donn√©es principal en anglais (en supposant que \'core_en\' est une configuration)
if "core_en" in configs:
    core_dataset = load_dataset("vincentkoc/tiny_qa_benchmark_pp", name="core_en", split="train")
    print(f"\\nCharg√© {len(core_dataset)} exemples depuis core_en :")
    # print(core_dataset[0]) # Imprimer le premier exemple
else:
    print("\\nConfiguration \'core_en\' non trouv√©e.")

# Charger un pack synth√©tique sp√©cifique (ex. un pack en fran√ßais)
# Remplacez \'pack_fr_40\' par un nom de configuration r√©el de la liste `configs`
example_pack_name = "pack_fr_40" # ou un autre nom de configuration valide
if example_pack_name in configs:
    synthetic_pack = load_dataset("vincentkoc/tiny_qa_benchmark_pp", name=example_pack_name, split="train")
    print(f"\\nCharg√© {len(synthetic_pack)} exemples depuis {example_pack_name} :")
    # print(synthetic_pack[0]) # Imprimer le premier exemple
else:
    print(f"\\nConfiguration \'{example_pack_name}\' non trouv√©e. Veuillez choisir parmi les configurations disponibles.")

```

Pour plus d'informations d√©taill√©es sur les jeux de donn√©es, y compris leur structure et leurs licences sp√©cifiques, veuillez consulter les fichiers README dans le r√©pertoire `data/` (c.-√†-d. `data/README.md`, `data/core_en/README.md` et `data/packs/README.md`).

## Structure du D√©p√¥t

*   `data/` : Contient les jeux de donn√©es QA.
    *   `core_en/` : Le jeu de donn√©es original de 52 √©l√©ments en anglais √©labor√© √† la main.
    *   `packs/` : Packs de donn√©es multilingues et th√©matiques g√©n√©r√©s synth√©tiquement.
*   `tools/` : Contient des scripts pour la g√©n√©ration et l'√©valuation de jeux de donn√©es.
    *   `generator/` : Le g√©n√©rateur de jeux de donn√©es QA synth√©tiques.
    *   `eval/` : Scripts et utilitaires pour √©valuer les mod√®les par rapport aux jeux de donn√©es TQB++.
*   `paper/` : La source LaTeX et les fichiers associ√©s pour l'article de recherche.
*   `metadata/` : Fichiers de m√©tadonn√©es Croissant JSON-LD pour les jeux de donn√©es.
*   `LICENSE` : Licence principale pour la base de code (Apache-2.0).
*   `LICENCE.data_packs.md` : Licence personnalis√©e pour les packs de donn√©es g√©n√©r√©s synth√©tiquement.
*   `LICENCE.paper.md` : Licence pour le contenu de l'article.

## Sc√©narios d'Utilisation

TQB++ est con√ßu pour divers flux de travail LLMOps et d'√©valuation :

*   **Tests de Pipeline CI/CD :** √Ä utiliser comme test unitaire avec des outils de test LLM pour les services LLM afin de d√©tecter les r√©gressions.
*   **Ing√©nierie des Prompts et D√©veloppement d'Agents :** Obtenez un retour rapide lors de l'it√©ration sur les prompts ou les conceptions d'agents.
*   **Int√©gration du Harnais d'√âvaluation :** Con√ßu pour une utilisation transparente avec les harnais d'√©valuation. Encodez-le en tant que YAML OpenAI Evals (voir `intergrations/openai-evals/README.md`) ou un jeu de donn√©es Opik pour le suivi du tableau de bord et une √©valuation LLM robuste. Le dossier `intergrations/` fournit plus de d√©tails sur le support pr√™t √† l'emploi disponible.
*   **D√©tection de D√©rive Multilingue :** Surveillez les r√©gressions de localisation √† l'aide des packs multilingues TQB++.
*   **Tests Adaptatifs :** Synth√©tisez de nouveaux micro-benchmarks √† la vol√©e adapt√©s √† des fonctionnalit√©s sp√©cifiques ou √† des d√©rives de donn√©es.
*   **Surveillance de la Dynamique de Fine-tuning :** Suivez l'√©rosion des connaissances ou les changements de capacit√© involontaires pendant le fine-tuning.

## Citation

Si vous utilisez TQB++ dans vos recherches ou votre travail, veuillez citer le TQB original et l'article TQB++ :

```bibtex
% Ce jeu de donn√©es synth√©tique et g√©n√©rateur
@misc{koctinyqabenchmarkpp,
  author       = {Vincent Koc},
  title        = {Tiny QA Benchmark++ (TQB++) Jeux de Donn√©es et Bo√Æte √† Outils},
  year         = {2025},
  publisher    = {Hugging Face & GitHub},
  doi          = {10.57967/hf/5531},
  howpublished = {\\url{https://huggingface.co/datasets/vincentkoc/tiny_qa_benchmark_pp}},
  note         = {Voir aussi : \\url{https://github.com/vincentkoc/tiny_qa_benchmark_pp}}
}

% Article TQB++
@misc{koc2025tinyqabenchmarkultralightweight,
      title={Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation}, 
      author={Vincent Koc},
      year={2025},
      eprint={2505.12058},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.12058}
}

% core_en.json original (52 en en)
@misc{koctinyqabenchmark_original,
  author       = {Vincent Koc},
  title        = {tiny_qa_benchmark},
  year         = {2025},
  publisher    = {Hugging Face},
  journal      = {Hugging Face Hub},
  doi          = {10.57967/hf/5417},
  url          = {https://huggingface.co/datasets/vincentkoc/tiny_qa_benchmark}
}
```

<!-- % Placeholder pour la citation de l'article TQB++ JMLR - √† mettre √† jour d√®s que disponible
@article{koc2025tqb_pp,
  author       = {Vincent Koc},
  title        = {Tiny QA Benchmark$^{++}$: Micro Gold Dataset with Synthetic Multilingual Generation for Rapid LLMOps Smoke Tests},
  journal      = {Journal of Machine Learning Research (en attente)},
  year         = {2025},
  volume       = {XX},
  number       = {X},
  pages        = {X-XX},
  url          = {http://jmlr.org/papers/vXX/koc25a.html} % URL d'exemple
} -->

## Licence
Le code de ce d√©p√¥t (y compris le g√©n√©rateur et les scripts d'√©valuation) ainsi que le jeu de donn√©es `data/core_en` et tout ce qui n'est pas mentionn√© avec une licence sont sous licence Apache License 2.0. Consultez le fichier `LICENSE` pour plus de d√©tails.

Les packs de jeux de donn√©es g√©n√©r√©s synth√©tiquement dans `data/packs/` sont distribu√©s sous une licence personnalis√©e "√âvaluation Uniquement, Non Commercial, Sans D√©riv√©s". Consultez `LICENCE.data_packs.md` pour plus de d√©tails.

Les fichiers de m√©tadonn√©es Croissant JSON-LD dans `metadata/` sont disponibles sous CC0-1.0.

Le contenu de l'article dans `paper/` est soumis √† ses propres termes de licence, d√©taill√©s dans `LICENCE.paper.md`. 